{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6d073-1157-4f34-8628-d1474ef97d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import d3rlpy\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from env.atari.represented_atari_game import GymCompatWrapper2\n",
    "from online_main import OneHotWrapper\n",
    "from pretrain_from_llm import get_llm_data_paths\n",
    "from vis_utils import *\n",
    "\n",
    "# hyperparams = {\n",
    "#     \"env\": \"CliffWalking-v0\",  # CartPole-v0, Pendulum-v1, MountainCar-v0, FrozenLake-v1, CliffWalking-v0, RepresentedPong-v0\n",
    "#     \"n_online_eps\": 170,  # 10-5990 for mountainCar, 30-120 for CartPole\n",
    "hyperparams = {\n",
    "    \"env\": \"FrozenLake-v1\",  # CartPole-v0, Pendulum-v1, MountainCar-v0, FrozenLake-v1, CliffWalking-v0, RepresentedPong-v0\n",
    "    \"n_online_eps\": 120,  # 10-5990 for mountainCar, 30-120 for CartPole\n",
    "    \"n_pretrain_eps\": 30,\n",
    "    \"seed\": 42069,\n",
    "    \"max_episode_len\": 200,  # Around 10h per 100k steps in Leviathan server\n",
    "    \"eps\": 0.1,  # epsilon for exploration\n",
    "    \"n_exp\": 5,\n",
    "    \"gpu\": True,  # True if use GPU to train with d3rlpy\n",
    "    \"buffer_size\": 100000,  # Test with 100k, 200k, 500k. 1M might be too much\n",
    "    \"data_path\": None,  #'data/CartPole_Qwen2.5-7B-Instruct_Neps_10_20250406040150.pkl',\n",
    "    \"model_path\": None,  #'d3rlpy_loss/DoubleDQN_online_20250331153346/model_600000.d3',\n",
    "    \"batch_size\": 256,  # Test smaller batch size: 32, 64. May be noisier\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"gamma\": 0.99,\n",
    "    \"target_update_interval\": 1000,  # Test with 1k, 2k, 5k\n",
    "    \"smooth\": 10,  # For plotting, smooth the curve with this window size\n",
    "    # \"n_episodes\": 150,\n",
    "}\n",
    "\n",
    "hyperparams[\"n_episodes\"] = hyperparams[\"n_pretrain_eps\"] + hyperparams[\"n_online_eps\"]\n",
    "# assert (\n",
    "#     hyperparams[\"n_episodes\"]\n",
    "#     == hyperparams[\"n_pretrain_eps\"] + hyperparams[\"n_online_eps\"]\n",
    "# ), \"Check n_episodes=n_pretrain_eps+n_online_eps\"\n",
    "\n",
    "if \"Represented\" in hyperparams[\"env\"]:\n",
    "    env = GymCompatWrapper2(gym.make(hyperparams[\"env\"]))\n",
    "    eval_env = GymCompatWrapper2(gym.make(hyperparams[\"env\"]))\n",
    "elif isinstance(gym.make(hyperparams[\"env\"]).observation_space, gym.spaces.Discrete):\n",
    "    env = OneHotWrapper(gym.make(hyperparams[\"env\"]))\n",
    "    eval_env = OneHotWrapper(gym.make(hyperparams[\"env\"]))\n",
    "else:\n",
    "    env = gym.make(hyperparams[\"env\"])\n",
    "    eval_env = gym.make(hyperparams[\"env\"])\n",
    "\n",
    "# fix seed\n",
    "d3rlpy.seed(hyperparams[\"seed\"])\n",
    "d3rlpy.envs.seed_env(env, hyperparams[\"seed\"])\n",
    "d3rlpy.envs.seed_env(eval_env, hyperparams[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700acf4-9af5-46f8-9fcf-24e3b7d8d2cd",
   "metadata": {},
   "source": [
    "### Static/pretrain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a0488c-3f4d-43e2-a0c2-fd3a0d88b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_7b, path_32b = get_llm_data_paths(hyperparams[\"env\"])\n",
    "\n",
    "with open(path_7b, \"rb\") as file:\n",
    "    Qwen_7B_dataset = pickle.load(file)\n",
    "with open(path_32b, \"rb\") as file:\n",
    "    Qwen_32B_dataset = pickle.load(file)\n",
    "\n",
    "Qwen_32B_rewards = []\n",
    "for i in range(hyperparams[\"n_pretrain_eps\"]):\n",
    "    Qwen_32B_rewards.append(Qwen_32B_dataset.episodes[i].compute_return())\n",
    "Qwen_7B_rewards = []\n",
    "for i in range(hyperparams[\"n_pretrain_eps\"]):\n",
    "    Qwen_7B_rewards.append(Qwen_7B_dataset.episodes[i].compute_return())\n",
    "\n",
    "Qwen_7B = hyperparams[\"n_episodes\"] * [np.mean(Qwen_7B_rewards)]\n",
    "Qwen_32B = hyperparams[\"n_episodes\"] * [np.mean(Qwen_32B_rewards)]\n",
    "\n",
    "print(\"Qwen_32B: \", np.mean(Qwen_7B_rewards))\n",
    "print(\"Qwen_7B: \", np.mean(Qwen_32B_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env_name = hyperparams[\"env\"].split(\"-\")[0]\n",
    "    if env_name == \"Pendulum\":\n",
    "        path_SFT = f\"data/{env_name}_Qwen2.5-7B-Instruct_Neps_{hyperparams['n_pretrain_eps']}_20250422210707SFT.pkl\"\n",
    "    elif env_name == \"CliffWalking\":\n",
    "        path_SFT = f\"data/{env_name}_Qwen2.5-7B-Instruct_Neps_{hyperparams['n_pretrain_eps']}_20250505135458SFT.pkl\"\n",
    "        # path_SFT = f\"data/{env_name}_Qwen2.5-7B-Instruct_Neps_{hyperparams['n_pretrain_eps']}_20250421175530SFT.pkl\" #CliffWalking typo\n",
    "    elif env_name == \"FrozenLake\":\n",
    "        path_SFT = f\"data/{env_name}_Qwen2.5-7B-Instruct_Neps_{hyperparams['n_pretrain_eps']}_20250501013018SFT.pkl\"\n",
    "        # path_SFT = f\"data/{env_name}_Qwen2.5-7B-Instruct_Neps_{hyperparams['n_pretrain_eps']}_20250421120400SFT.pkl\" #FrozenLake typo\n",
    "\n",
    "    with open(path_SFT, \"rb\") as file:\n",
    "        Qwen_7B_SFT_dataset = pickle.load(file)\n",
    "\n",
    "    Qwen_7B_SFT_rewards = []\n",
    "    for i in range(hyperparams[\"n_pretrain_eps\"]):\n",
    "        Qwen_7B_SFT_rewards.append(Qwen_7B_SFT_dataset.episodes[i].compute_return())\n",
    "    Qwen_7B_SFT = Qwen_7B_SFT_rewards + hyperparams[\"n_online_eps\"] * [\n",
    "        np.mean(Qwen_7B_SFT_rewards)\n",
    "    ]\n",
    "    print(\"Qwen_7B_SFT: \", np.mean(Qwen_7B_SFT_rewards))\n",
    "except:\n",
    "    Qwen_7B_SFT = None\n",
    "try:\n",
    "    path_DS_7B = f\"data/{hyperparams['env'].split('-')[0]}_DeepSeek-R1-Distill-Qwen-7B_Neps_{hyperparams['n_pretrain_eps']}_20250502071148.pkl\"  # FrozenLake\n",
    "    path_DS_14B = f\"data/{hyperparams['env'].split('-')[0]}_DeepSeek-R1-Distill-Qwen-14B_Neps_{hyperparams['n_pretrain_eps']}_20250502084016.pkl\"  # FrozenLake\n",
    "    # path_DS_7B = f\"data/{hyperparams['env'].split('-')[0]}_DeepSeek-R1-Distill-Qwen-7B_Neps_{hyperparams['n_pretrain_eps']}_20250419172821.pkl\" #FrozenLake typo\n",
    "    # path_DS_14B = f\"data/{hyperparams['env'].split('-')[0]}_DeepSeek-R1-Distill-Qwen-14B_Neps_{hyperparams['n_pretrain_eps']}_20250422000525.pkl\" #FrozenLake typo\n",
    "    with open(path_DS_7B, \"rb\") as file:\n",
    "        DS_7B_dataset = pickle.load(file)\n",
    "    with open(path_DS_14B, \"rb\") as file:\n",
    "        DS_14B_dataset = pickle.load(file)\n",
    "    DS_7B_rewards = []\n",
    "    for i in range(hyperparams[\"n_pretrain_eps\"]):\n",
    "        DS_7B_rewards.append(DS_7B_dataset.episodes[i].compute_return())\n",
    "    DS_14B_rewards = []\n",
    "    for i in range(hyperparams[\"n_pretrain_eps\"]):\n",
    "        DS_14B_rewards.append(DS_14B_dataset.episodes[i].compute_return())\n",
    "\n",
    "    DS_7B = hyperparams[\"n_episodes\"] * [np.mean(DS_7B_rewards)]\n",
    "    DS_14B = hyperparams[\"n_episodes\"] * [np.mean(DS_14B_rewards)]\n",
    "    # DS_7B = DS_7B_rewards + hyperparams[\"n_online_eps\"] * [np.mean(DS_7B_rewards)]\n",
    "    # DS_14B = DS_14B_rewards + hyperparams[\"n_online_eps\"] * [np.mean(DS_14B_rewards)]\n",
    "\n",
    "    print(\"DS_7B: \", np.mean(DS_7B_rewards))\n",
    "    print(\"DS_14B: \", np.mean(DS_14B_rewards))\n",
    "except:\n",
    "    DS_7B = None\n",
    "    DS_14B = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa3e7b",
   "metadata": {},
   "source": [
    "### Generate and evaluate random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1009e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the random policy manually\n",
    "random_rewards = []\n",
    "for _ in range(hyperparams[\"n_pretrain_eps\"]):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    count = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        count += 1\n",
    "        if count >= hyperparams[\"max_episode_len\"]:\n",
    "            break\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "mean_random = np.ones(\n",
    "    hyperparams[\"n_pretrain_eps\"] + hyperparams[\"n_online_eps\"]\n",
    ") * np.mean(random_rewards)\n",
    "# Print the average reward of the random policy\n",
    "print(f\"Average reward for random policy: {mean_random[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778df60f-a837-4aec-a95a-7bb92e7ea615",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a300edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cache = extract_data(hyperparams, Qwen_7B, Qwen_32B, DS_7B, DS_14B, Qwen_7B_SFT, mean_random)\n",
    "# plot_main(hyperparams, all_cache, Qwen_7B, Qwen_32B, mean_random)\n",
    "# plot_pretrain(hyperparams, all_cache)\n",
    "# # plot_pretrain_big(hyperparams, all_cache)\n",
    "# plot_mix(hyperparams, all_cache)\n",
    "# plot_model_size(hyperparams, all_cache)\n",
    "# plot_pretrain_step(hyperparams, all_cache)\n",
    "# plot_pretrain_eps(hyperparams, all_cache)\n",
    "# # plot_sft_lcot(hyperparams, all_cache)\n",
    "# plot_sft_lcot_big(hyperparams, all_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea8930",
   "metadata": {},
   "source": [
    "## Coverage and Transfer Coefficient (FrozenLake and CliffWalking only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad043e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_online_eval_eps=10\n",
    "merge_time_step=False #False to calculate the transfer coefficient in the original formula, True to merge the time step to compare coverage of the full sequence\n",
    "def print_results(offline_dataset, online_dataset, n_pretrain_eps, n_online_eval_eps, text, merge_time_step=False):\n",
    "    max_transfer_coef, max_key, (no_diff, no_online_keys, no_offline_keys, no_diff2) = datasets_to_transfer_coef_upper_bound(online_dataset, offline_dataset, n_pretrain_eps, n_online_eval_eps, merge_time_step=merge_time_step)\n",
    "    if max_transfer_coef != np.inf:\n",
    "        print(f\"{text}: Max transfer coef: {max_transfer_coef} at step {max_key[0]}, state {max_key[1]}, action {max_key[2]}\")\n",
    "    else:\n",
    "        print(f\"{text}: Max transfer coef: inf\")\n",
    "    print(f\"No of online keys: {no_online_keys}. No of offline keys: {no_offline_keys}. No online not offline: {no_diff}. No offline not online: {no_diff2}\")\n",
    "if hyperparams[\"env\"] == \"CliffWalking-v0\" or hyperparams[\"env\"] == \"FrozenLake-v1\":\n",
    "    for i in range(hyperparams[\"n_exp\"]):\n",
    "        print(f\"Experiment {i}:\")\n",
    "        for n_eps in [30]:\n",
    "        # for n_eps in [10, 20, 30]:\n",
    "            Qwen_7B_LORO_1000_Neps_dataset = all_cache[f\"pretrain_7b_1000_{n_eps}_{i}_dataset\"]\n",
    "            Qwen_32B_LORO_1000_Neps_dataset = all_cache[f\"pretrain_32b_1000_{n_eps}_{i}_dataset\"]\n",
    "            on_pol_1000_Neps_dataset = all_cache[f\"on_pol_1000_{n_eps}_{i}_dataset\"]\n",
    "            on_pol_1000_Neps_offline_dataset = all_cache[f\"on_pol_1000_{n_eps}_{i}_offline_dataset\"]\n",
    "            rand_1000_Neps_dataset = all_cache[f\"rand_1000_{n_eps}_{i}_dataset\"]\n",
    "            rand_1000_Neps_offline_dataset = all_cache[f\"rand_1000_{n_eps}_{i}_offline_dataset\"]\n",
    "\n",
    "            # print_results(Qwen_7B_dataset, Qwen_32B_dataset, hyperparams[\"n_pretrain_eps\"], n_online_eval_eps, f\"Qwen 7B vs Qwen 32B\", merge_time_step)\n",
    "            print_results(Qwen_7B_dataset, Qwen_7B_LORO_1000_Neps_dataset, hyperparams[\"n_pretrain_eps\"], n_online_eval_eps, f\"Qwen 7B LORO 1000 {n_eps}\", merge_time_step)\n",
    "            print_results(Qwen_32B_dataset, Qwen_32B_LORO_1000_Neps_dataset, hyperparams[\"n_pretrain_eps\"], n_online_eval_eps, f\"Qwen 32B LORO 1000 {n_eps}\", merge_time_step)\n",
    "            print_results(on_pol_1000_Neps_offline_dataset, on_pol_1000_Neps_dataset, hyperparams[\"n_pretrain_eps\"], n_online_eval_eps, f\"On-policy 1000 Neps {n_eps}\", merge_time_step)\n",
    "            print_results(rand_1000_Neps_offline_dataset, rand_1000_Neps_dataset, hyperparams[\"n_pretrain_eps\"], n_online_eval_eps, f\"Random 1000 Neps {n_eps}\", merge_time_step)\n",
    "        # print(\"--------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed36b8",
   "metadata": {},
   "source": [
    "## For visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    plot_coverage_heatmap(Qwen_32B_dataset, hyperparams[\"n_pretrain_eps\"], i, model_name=\"Qwen 32B\", figsize=(10, 8), cmap='Blues', annot=True, fmt='.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b91701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamagym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
